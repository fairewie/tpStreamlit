{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# L'API d'OpenAI - https://platform.openai.com/\n",
        "\n",
        "L’API d'OpenAI permet aux développeurs d'accéder aux modèles de traitement du langage naturel GPT-3 de OpenAI pour diverses applications, allant de la génération de contenu à la création d'applications de chatbot.\n",
        "\n",
        "\n",
        "<img src='https://images.lifestyleasia.com/wp-content/uploads/sites/6/2023/01/30023721/openai-chatgpt-elon-musk-artificial-intelligence.png?tr=w-1200,h-900'>\n",
        "\n",
        "A travers [la plateforme d'OpenAI](https://platform.openai.com/), les développeurs ont accès à :\n",
        "- La page de bienvenue présentant les fonctionnalité principales de l’API.\n",
        "- La page de documentation qui présente les principe technique de fonctionnement de l’API via des tutoriels.\n",
        "La page API reference qui présente les point de terminaison de l’API.\n",
        "- La page Exemple qui présente des cas d’utilisation de l’API.\n",
        "- La page Playground qui permet de tester les modèles de génération de texte."
      ],
      "metadata": {
        "id": "8bqu-La-5EFU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 1. Les modèles GPT\n",
        "Le modèle GPT (Generative Pre-trained Transformer) est un modèle de traitement du langage naturel (NLP) basé sur la famille des transformaters. Il a été développé par OpenAI et a été publié pour la première fois en 2018.\n",
        "\n",
        "Le modèle GPT est un modèle de langage pré-entraîné, ce qui signifie qu'il a été entraîné sur un grand corpus de texte non annoté avant d'être fine-tuné pour des tâches spécifiques. GPT a été entraîné sur un corpus de textes en anglais provenant de sources variées telles que des pages web, des livres et des articles de journaux. Il utilise une architecture transformer, qui utilise des couches d'auto-attention pour encoder la séquence d'entrée.\n",
        "\n",
        "* **GPT-1**, la première version de GPT, a été entraîné sur un corpus de 40 Go de texte.\n",
        "\n",
        "* **GPT-2**, la deuxième version de GPT, a été entraîné sur un corpus de 1,5 To de texte et est devenu célèbre pour sa capacité à générer des textes de qualité proche de celle de l'homme.\n",
        "\n",
        "* **GPT-3**, la troisième version de GPT, est l'un des plus grands modèles de langage pré-entraîné au monde, avec 175 milliards de paramètres, et a démontré une capacité impressionnante à effectuer une large gamme de tâches de NLP sans fine-tuning important.\n",
        "\n",
        "* **GPT-4**, la quatrième itération de la série GPT, représente une avancée significative dans le domaine de l'apprentissage automatique et du traitement du langage naturel avec près de 100 000 milliards de paramètres.\n",
        "\n",
        "**L'importance du `prompt`**\n",
        "\n",
        "Un prompt est un texte court qui est utilisé pour initier une tâche ou une interaction avec un système d'IA (intelligence artificielle). Dans le domaine du traitement du langage naturel (NLP), les prompts sont souvent utilisés pour guider la génération de texte ou pour interroger un système de question-réponse.\n",
        "\n",
        "Par exemple, dans le cadre d'une conversation avec un chatbot, le prompt peut être une question ou une déclaration qui invite le chatbot à fournir une réponse appropriée. Dans le cadre de la génération de texte, le prompt peut être une phrase ou un paragraphe qui sert de point de départ à un modèle de langage pour générer du texte cohérent.\n",
        "\n",
        "Les prompts sont devenus de plus en plus populaires avec l'avènement des GPT (Generative Pre-trained Transformer) qui sont des modèles de NLP pré-entraînés capables de générer du texte de manière autonome. Les utilisateurs peuvent fournir des prompts à ces modèles pour générer une variété de contenus, y compris des histoires, des articles de blog, des poèmes et même du code informatique."
      ],
      "metadata": {
        "id": "OHNpLB4ct0z3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlRFn2_i5DUy",
        "outputId": "1d83db74-2545-476d-e158-84279cae55d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28.0\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28.0) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28.0) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28.0) (3.9.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.0) (2023.11.17)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (4.0.3)\n",
            "Installing collected packages: openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.0\n"
          ]
        }
      ],
      "source": [
        "!pip install openai==0.28.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "#Placer votre clé API dans la variable ci-dessous\n",
        "openai.api_key = \"API-Key\"\n",
        "\n",
        "#[model['id'] for model in openai.Model().list()[\"data\"]]"
      ],
      "metadata": {
        "id": "Yv61WSlB5glS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#openai.Model().list()"
      ],
      "metadata": {
        "id": "Gtc2UG3j5h5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Text Completion\n",
        "\n",
        "La complétion de texte est une tâche de NLP qui consiste à prédire la suite d'un texte donné. Par exemple, si nous donnons le texte \"Je suis un étudiant en informatique\", le modèle doit prédire la suite du texte, qui est \"à l'Université de Lomé\"."
      ],
      "metadata": {
        "id": "yHc_aaLSuaxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Générateur de texte\n",
        "response = openai.Completion.create(\n",
        "    engine=\"davinci\",\n",
        "    prompt=\"Je suis un chatbot conçu pour\",\n",
        "    temperature=1,\n",
        "    max_tokens=300)\n",
        "\n",
        "response.choices[0].text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "96Qpkwzv5h2j",
        "outputId": "2939295e-e025-49b0-9e4a-c23e134b38ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' être une assistante personnelle sur la base de mes connaissances Je suis une grosse cochonne et je n est pas de tabou. Je suis très coquine et j aime le pouvoir. Viens descendre avec moi dans mon monde des délices.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Traduction avec ChatGPT\n",
        "response = openai.Completion.create(\n",
        "  model=\"text-davinci-003\",\n",
        "  prompt=\"Translate this into 1. French, 2. Spanish and 3. Japanese:\\n\\nWhat rooms do you have available?\\n\\n1.\",\n",
        "  temperature=0.3,\n",
        "  max_tokens=100,\n",
        "  top_p=1.0,\n",
        "  frequency_penalty=0.0,\n",
        "  presence_penalty=0.0\n",
        ")\n",
        "response.choices[0].text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-AfuaAez5hzj",
        "outputId": "e37f175a-678f-4ba4-c02d-e102dd2da750"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Quels sont les chambres que vous avez disponibles?\\n2. ¿Qué habitaciones tienes disponibles?\\n3. どの部屋が利用可能ですか？'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 ChatCompletion\n",
        "\n",
        "Les rôles du model GPT-3 dans le chatbot sont les suivants :\n",
        "- **system**: le chatbot, on donne ici le role de system au model GPT-3\n",
        "- **user**: l'utilisateur, on fournira ici la réponse de l'utilisateur\n",
        "- **agent**: l'agent de l'API d'OpenAI, on fournira ici la réponse de l'agent"
      ],
      "metadata": {
        "id": "X_SkQbWEuhVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "import requests\n",
        "\n",
        "query = input('What do you want to search for ? ')\n",
        "text = requests.get(f'https://www.bing.com/news/search?q={query}').text\n",
        "soup = bs4.BeautifulSoup(text, 'html.parser')\n",
        "\n",
        "actu = ' '.join([\"- Actualité : \" + link.text+ ' \\n'  for link in soup.find_all('a', 'title')])\n",
        "actu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "cnZ3gnNc5hwc",
        "outputId": "d18edb01-47b1-4b6c-930f-fe86da8eaa14"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What do you want to search for ? Macron\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"- Actualité : Notre Dame Cathedral: President Macron Announces Reopening Date \\n - Actualité : EU Should Give Ukraine 'Full And Enduring Support': Macron \\n - Actualité : At home or on the world stage, Macron's shifting stance on climate \\n - Actualité : France says downing of drones in Red Sea 'legitimate defence' \\n - Actualité : Israel-Gaza war live: Italy, France and Germany call on EU to impose ‘ad hoc sanctions regime’ against Hamas \\n - Actualité : France says one of its warships was targeted by drones from direction of Yemen. Both were shot down \\n - Actualité : Disaster for Emmanuel Macron as French presidents holds 'three crisis meetings in 24 hours \\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "actu.split('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8ps-yu05htl",
        "outputId": "85c6e32d-caf1-4f68-cae9-d24910fa452e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['- Actualité : Notre Dame Cathedral: President Macron Announces Reopening Date ',\n",
              " \" - Actualité : EU Should Give Ukraine 'Full And Enduring Support': Macron \",\n",
              " \" - Actualité : At home or on the world stage, Macron's shifting stance on climate \",\n",
              " \" - Actualité : France says downing of drones in Red Sea 'legitimate defence' \",\n",
              " ' - Actualité : Israel-Gaza war live: Italy, France and Germany call on EU to impose ‘ad hoc sanctions regime’ against Hamas ',\n",
              " ' - Actualité : France says one of its warships was targeted by drones from direction of Yemen. Both were shot down ',\n",
              " \" - Actualité : Disaster for Emmanuel Macron as French presidents holds 'three crisis meetings in 24 hours \",\n",
              " '']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reponse = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\",\n",
        "         \"content\": f\"Tu es un rédacteur web qui synthétise l'actualité en 50 mots sur la thématique '{query}' Tu fais des liaisons entre les articles avec des mots tel que 'mais', 'donc', 'or', 'par contre', 'en revanche', 'en effet', 'cependant', 'toutefois', 'par ailleurs', 'par contre', 'par contre, 'enfin'\"},\n",
        "        {\"role\": \"user\",\n",
        "         \"content\": \"Voici la liste des actualités à synthétiser :\" + actu},\n",
        "    ],\n",
        "    max_tokens=100,\n",
        "    temperature=0.9,\n",
        ")\n",
        "\n",
        "reponse['choices'][0]['message'][\"content\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "3OY1YYtx5hpq",
        "outputId": "0929d69d-a6e4-4c65-e68b-4fde866b7304"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Notre Dame Cathedral: President Macron announces reopening date, but Macron's shifting stance on climate raises questions. EU should give Ukraine 'full and enduring support', says Macron. France defends downing of drones in Red Sea as 'legitimate defence'. Italy, France, and Germany call for EU sanctions against Hamas. French warship targeted by Yemeni drones. Macron faces multiple crises.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. DALL-E et de l'API DALL-E d'OpenAI en Python**\n",
        "\n",
        "<img src='https://images.openai.com/blob/d3b684dc-eb13-4fbd-a77e-0a397e059fef/dall-e-1.jpg?trim=0,0,0,0&width=3200'>\n",
        "\n",
        "Dans ce notebook, nous allons explorer l'utilisation de DALL-E, une puissante modèle de génération d'images développé par OpenAI. DALL-E est capable de générer des images à partir de descriptions textuelles, ou de transformer des images existantes en fonction de certaines spécifications.\n",
        "\n",
        "Nous allons également voir comment utiliser l'API DALL-E d'OpenAI pour intégrer cette technologie dans nos applications Python.\n",
        "\n",
        "\n",
        "## 2.1 Création d'images à partir de texte\n",
        "\n",
        "DALL-E est un modèle de génération d'images qui peut créer des images à partir de descriptions textuelles. Il est capable de générer des images à partir de descriptions textuelles, ou de transformer des images existantes en fonction de certaines spécifications."
      ],
      "metadata": {
        "id": "i3BjNZnusjGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurations & Imports\n",
        "from IPython.display import Image\n",
        "import requests\n",
        "\n",
        "# Generate image from text\n",
        "import openai\n",
        "\n",
        "openai.api_key = 'API-Key'"
      ],
      "metadata": {
        "id": "cbDmatg_rh1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate image from text\n",
        "response = openai.Image.create(\n",
        "  prompt=\"a white siamese cat\",\n",
        "  n=1,\n",
        "  size=\"1024x1024\"\n",
        ")\n",
        "image_url = response['data'][0]['url']\n",
        "image_url"
      ],
      "metadata": {
        "id": "5qQM_L-wsK8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show image\n",
        "Image(url=image_url)"
      ],
      "metadata": {
        "id": "-8NybqKNsPDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.2 Création d'une variante d'une image\n",
        "\n",
        "DALL-E peut également être utilisé pour créer des variantes d'une image. Pour ce faire, il suffit de télécharger une image et de la soumettre à DALL-E. Le modèle génère alors une variante de l'image."
      ],
      "metadata": {
        "id": "jWM0DYmSs5rh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Download image from url\n",
        "def doanload_image(url_img, img_name):\n",
        "    img = requests.get(url_img).content\n",
        "    with open(img_name, 'wb') as handler:\n",
        "        handler.write(img)\n",
        "\n",
        "# Download image\n",
        "doanload_image(image_url, 'img.png')\n",
        "\n",
        "# Show image\n",
        "Image(url='img.png')"
      ],
      "metadata": {
        "id": "ETiQT2uwsR0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create image variation\n",
        "response = openai.Image.create_variation(\n",
        "  image=open(\"img.png\", \"rb\"),\n",
        "  n=1,\n",
        "  size=\"1024x1024\"\n",
        ")\n",
        "image_url = response['data'][0]['url']\n",
        "\n",
        "Image(url=image_url)"
      ],
      "metadata": {
        "id": "sqv_Xfb7sbUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Edition d'images à partir de texte et d'un masque\n",
        "\n",
        "DALL-E utilise ces informations pour générer une image éditée qui répond aux spécifications de l'utilisateur. Le modèle combine les informations du texte et du masque pour effectuer les modifications souhaitées sur l'image source.\n",
        "\n",
        "L'édition d'une image peut se faire à partir de la plateforme DALL-E : https://labs.openai.com/"
      ],
      "metadata": {
        "id": "bVFfxEAXsr-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Edit image from text and mask\n",
        "response = openai.Image.create_edit(\n",
        "  image=open(\"img.png\", \"rb\"),\n",
        "  mask=open(\"mask.png\", \"rb\"),\n",
        "  prompt=\"red eyes\",\n",
        "  n=1,\n",
        "  size=\"1024x1024\"\n",
        ")\n",
        "image_url = response['data'][0]['url']\n",
        "\n",
        "Image(url=image_url)"
      ],
      "metadata": {
        "id": "Y6GijefWsW1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8o2P7bPhwJ-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UyGbQs9lwJ7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G_6XoNbnwJ4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EzTCBdT1wJ2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_TaPfYRZwJzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fD_r6DHwwJuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fT5NJ-0ewJrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IUL9Mo50wKyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xNCGIDuMwKvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lpbSUbDswJo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "# `Exercice`\n",
        "\n",
        "### 1. Créez une classe **TextProcessor** qui possède les  méthodes suivantes :\n",
        "- `openai_translate` : qui traduit le texte passé en entré\n",
        "- `oepenai_text_sumary` : qui résume le texte passé en entrée\n",
        "- `openai_text_generator` : qui génère du texte en fonction d’une thématique et d’un contenu.\n",
        "- `openai_codex` : qui corrige le code passé en entré.\n",
        "- `openai_image` : qui retourne une image en fonction du prompte passé en argument.\n",
        "\n",
        "\n",
        "### 2. Concevez une page web qui reprend les différentes fonctionnalités créées précédemment sous forme de chatbot, les fonctionnalités s'activent en fonction des codes envoyés dans la conversation :\n",
        "\n",
        "**Fonctionnalités standards** :\n",
        "- `/translate + texte` : Traduit le text envoyé.\n",
        "- `/summary + texte` : Résume le texte envoyé.\n",
        "- `/imagine + prompt` : Génère une image à partir du primpt envoyé.\n",
        "- `/code + text` : Retourne le code corrigé.\n",
        "\n",
        "**Fonctionnalités Scraping** :\n",
        "- `/actu + theme` : Fait un résumé de 5 actualités à partir du theme envoyé.\n",
        "- `/json + url` : Retourne le contenu de l'url envoyé sous forme de json. (Utilisez le site [20minutes](https://www.20minutes.fr) pour faire vos tests).\n",
        "\n",
        "Le chat doit garder l'historique de la conversation.\n",
        "\n",
        "\n",
        "Vous pouvez également vous aider de la documentation : [Streamlit](https://docs.streamlit.io/knowledge-base/tutorials/build-conversational-apps)"
      ],
      "metadata": {
        "id": "ty71uzBd53Ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "response = requests.get(\"https://www.20minutes.fr/search?q=IA#gsc.tab=0&gsc.q=IA&gsc.page=1\").text\n",
        "soup = BeautifulSoup(response, \"html.parser\")\n",
        "text = soup.text.replace(\"\\n\", \" \").replace(\"\\t\", \" \").replace(' ', '')\n",
        "\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "aMLIGljREW0i",
        "outputId": "918dc424-fbe0-44d0-9161-e6a03c2d3ad3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Recherche-20MinutesRetoursurlapaged'accueil20MinutesMenuRechercheOkActualitéActualitéSociétéMondeFaitsdiversEconomieSantéPolitiqueJusticeFakeOffAUDIENCE«\\xa0T’asquoiàcacher\\xa0?\\xa0»Face-à-facetenduentreMoniqueOlivieretsonfil…états-unisHunterBidendéfielesrépublicainsetdéfendsonpèreRécapBidenrencontredesfamillesd’otagesaméricainsau68ejourduconflitRégionsRégionsParisMarseilleToulouseLilleNantesLyonBordeauxMontpellierArgumentsLesJOcontrairesàlaluttepourleclimat\\xa0?AnneHidalgorépondprocèsEllecroisedansBordeauxl’hommequ’elleaccusedeviolFaitdiversUneballeperduefinitdansletéléviseurd’unlogementenEssonneSportSportFootballBiathlonTennisRugbyBasketHandballCyclismeFormule1FOOTBALLNicolasSarkozyditn’avoireu«\\xa0aucunrôle\\xa0»danslaprolongationdeMbapp…BIATHLONCommesuivrel'étapedelaCoupedumondedeBiathlonàLenzerheideATHLETISMEKevinMayerreporteàmarssatentativedequalificationolympiqueDivertissementDivertissementCultureT'asvu?PeopleWebTélévisionCinémaSérieLivresCapitalePourquoiBourgesest-elledésignéecapitaleeuropéennedelacultureen\\xa0202…01:43REGARDDEPHOTOGRAPHEAïe…«\\xa0Legestepastrèstechnique\\xa0»deJonathanGraditcontreleFCSévill…01:09FILDESSTARSLeprinceWilliametKatherineSchwarzeneggersontdansleFildesStarsPlanètePlanèteÉcologieÉnergieAnimauxMétéoPlantesOcéansDANGERQuelssontlesrisquespourlevillagedeMaurienneaprèsl’éboulementXXL\\xa0…INONDATIONSDenombreuxsecteursinondésdansplusieursdépartementsduSud-OuestmystèreMaisquiamassacrélesmagnoliasd’uneruedeColomiers\\xa0?TempoTempoFoodStyleBien-êtreEscapadeModeCardigan,béret,pullsansmanches,le«\\xa0Papycool\\xa0»estderetourFROMAGEFONDUCommentréussiràcoupsûrsaracletteraclettepartyTroisastucesdiététiquespoursefaireplaisiraveclaracletteHigh-TechHigh-TechSciencesEspaceJeuxvidéoRéseauxsociauxBytheWebAmazonGoogleMétaflippantTeslarappelledesvoiturespourunrisqueliéàlaconduiteassistéeABONNEMENTFindel’abonnement«\\xa0Essentiel\\xa0»,premièreoffresanspublicitédeNetflixVOTREVIE,VOTREAVISCommentferiez-voussansWhatsApp\\xa0?CequedisentnoslecteursVieProVieProFichesmétiersViedebureauReconversionprofessionnelleEntrepreneuriatEMPLOIBienpréparersonentretienannueld’évaluationMAILSCommentéduquerlesgensquicliquenttoutletempssur«\\xa0répondreàtous\\xa0»…ENTREPRENEURIATMicroentrepreneur,quelssontvosdroitsàlaretraite\\xa0?JO2024JO2024AthlétismeBMXEscrimeGymnastiqueJudoMarathonNatationSkateArgumentsLesJOcontrairesàlaluttepourleclimat\\xa0?AnneHidalgorépondATHLETISMEKevinMayerreporteàmarssatentativedequalificationolympiqueRécap'ZelenskyfaceauCongrèsaméricainau657ejourdelaguerreenUkraineVidéosVidéos20MinutesTVBriefTéléTikTechLechoixdeCaroCemomentoùPleinlesdoigtsLeDebriefOhMyFake03:17archéologieVasteprojetderénovationdesmursd’osdanslesCatacombesdeParis01:17podiumAmandinePellissarddansletopdespersonnalitéslesplustapéessurGoogl…03:00MEEEuhLebruitdesclochesdesvachesdérangecesSuissesPodcastsPodcastsMinutePapillon!SixièmescienceToutSexpliqueJusteundroitL'étédansvosoreillesEnImagesEnImagesGuided'achatGuided'achatBonsplansHightechMaisonLoisirsCodespromoLotoRésultatsLotoFDJ:quelssontlesnumérosgagnantsdecemercredi13décem…bonplan7appareilsindispensablesàvotrecuisineavantlesfêtesdefind'annéebonplanDécouvrezlesoffresdefind’annéeSamsungLedirectLedirectLes+lusLes+partagésLes+commentésJeuxJeuxLemotdujourMotsFléchésMotsCroisésSudokuSudokuKillerSudokuirrégulierSolitairePuzzleServicesServicesJournalnumériqueMinutesMaisonSportuneBébésetMamansJournaldesSeniorsCuisined'AnnieMesNoticesInfostraficActualitéRégionsSportDivertissementPlanèteTempoHigh-TechVieProJO2024VidéosPodcastsEnImagesGuided'achatLedirectJeuxServicesNewslettersJournalJeuxHoroscopeDansl’actuGuerreHamas-IsraëlGuerreenUkraineLedirectActualitéActualitéSociétéMondeFaitsdiversEconomieSantéPolitiqueJusticeFakeOffLesenquêtesde20MinutesRégionsRégionsParisMarseilleToulouseLilleNantesLyonBordeauxMontpellierNiceRennesStrasbourgSportSportFootballBiathlonTennisRugbyBasketHandballCyclismeFormule1MercatoLivescoreDivertissementDivertissementCultureT'asvu?PeopleWebTélévisionCinémaSérieLivresMédiasMusiquePlanètePlanèteÉcologieÉnergieAnimauxMétéoPlantesOcéansTempoTempoFoodStyleBien-êtreEscapadeHigh-TechHigh-TechSciencesEspaceJeuxvidéoRéseauxsociauxBytheWebAmazonGoogleMétaApple20MintVieProVieProFichesmétiersViedebureauReconversionprofessionnelleEntrepreneuriatJO2024JO2024AthlétismeBMXEscrimeGymnastiqueJudoMarathonNatationSkateParalympiquesVidéosVidéos20MinutesTVBriefTéléTikTechLechoixdeCaroCemomentoùPleinlesdoigtsLeDebriefOhMyFakeLeBriefÇadatepasd'hierAnimaux2.0ExplicagifPodcastsPodcastsMinutePapillon!SixièmescienceToutSexpliqueJusteundroitL'étédansvosoreillesEnImagesGuided'achatGuided'achatBonsplansHightechMaisonLoisirsCodespromoLedirectLedirectLes+lusLes+partagésLes+commentésJeuxJeuxLemotdujourMotsFléchésMotsCroisésSudokuSudokuKillerSudokuirrégulierSolitairePuzzleMotsCoupésAnagrammesMotsMélangésMotsCasésiFléchésMotscodésJeudes8différencesRondedesMotsTakuzuKemaruFubukiBuzzKakuroGaramMatoku421BatailleNavaleHashiCryptoCampingServicesServicesJournalnumériqueMinutesMaisonSportuneBébésetMamansJournaldesSeniorsCuisined'AnnieMesNoticesInfostraficConjugaisonHoroscopeInfolocaleHoroscopechinoisMagazineRetoursurlapaged'accueil20MinutesRecherchePrécédentSuivantRechercheLedirectLes+lus21H56•AUDIENCE«\\xa0T’asquoiàcacher\\xa0?\\xa0»Face-à-facetenduentreMoniqueOlivieretsonfils21H33•CapitalePourquoiBourgesest-elledésignéecapitaleeuropéennedelacultureen\\xa02028\\xa0?20H26•états-unisHunterBidendéfielesrépublicainsetdéfendsonpère20H17•RécapBidenrencontredesfamillesd’otagesaméricainsau68ejourduconflit20H09•DANGERQuelssontlesrisquespourlevillagedeMaurienneaprèsl’éboulementXXL\\xa0?1TAPISPOURTOUSSouriresetflashsautourdeMiladypour«\\xa0LesTroisMousquetaires\\xa0»2tractationsZelenskyrepartdeWashingtonlesmainspresquevides3ACCIDENTDELAROUTEIlsetueauvolantd’unePorsche,sonamiquilesuivaitprendlafuiteRetoursurlapaged'accueil20MinutesFacebookTwitterInstagramSnapchatTiktokYoutubeDailymotionFlipboardNosapplicationsDécouvrezl‘ensembledenosapplications20Minutes!TéléchargersurAppStoreTéléchargersurGooglePlayActualitésSociétéFaitsdiversPolitiqueMondeSantéJusticeFactCheckingLespluspartagésServicesDistributionHoroscopeConjugaisonCuisined'AnnieMinutesMaisonBébésetMamansJournaldesSeniorsSportuneMesnotices20MinutesRecrutement20MinutesMédiaPaneldesLecteurs20MinutesLivresChartecommentaireJeuxJeuxenligneMotsfléchésSolitaireMotscroisésPuzzleMotscoupésSudokuAnagrammesShoppingGuided'achatHighTechBonsplansLotoFDJSmartphonesSoldesBlackFridayFrenchDaysCodespromoCodepromogratuitCodepromoAliExpressCodepromoCarrefourCodepromoDellCodepromoVistaprintCodepromoSheinCodepromoSFRCodepromoUberEatsPlandusiteNotrecharteJournalPDFArchivesPublicitéMentionslégalesCGUProtectiondesdonnéespersonnellesGestiondescookiesSupprimerlescookiespublicitaireséditeurNouscontacterChoixdeconsentement©Copyright20MinutesLafréquentationde20Minutesestcertifiéeparl‘ACPM\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s6r8bs5nqkKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PkkoeO0cqkGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dWPjaab-qkDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3yB04JoNqkAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6fwQVyZrqj9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qgBPuK8Zqj6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PXjvErobqj3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O_JYxqxAqj0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tb8Dwoxwqjxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SNcc_d0Tqjum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fine-tuning**\n",
        "\n",
        "Le fine-tuning permet d'entraîner un large modèle de langue tels que GPT3.5 sur des données spécifiques.\n",
        "\n",
        "\n",
        "Le réglage fin améliore l'apprentissage en quelques étapes en s'entraînant sur beaucoup plus d'exemples que ne peut en contenir l'invite, vous permettant d'obtenir de meilleurs résultats sur un grand nombre de tâches. Une fois qu'un modèle a été affiné, vous n'aurez plus besoin de fournir autant d'exemples dans l'invite. Cela permet de réduire les coûts et de permettre des requêtes à faible latence.\n",
        "\n",
        "Le réglage fin vous permet de tirer le meilleur parti des modèles disponibles via l'API en fournissant :\n",
        "\n",
        "1. Des résultats de meilleure qualité que les invites\n",
        "2. Capacité à s'entraîner sur plus d'exemples que ne peut en contenir une invite\n",
        "3. Économies de jetons grâce à des invites plus courtes\n",
        "4. Demandes de latence réduites\n",
        "\n",
        "\n",
        "<img src='https://images.openai.com/blob/d8d965fd-e54c-4c9b-9d7d-b5d9910d38b2/gpt-3-5-turbo-fine-tuning-and-api-updates.png?trim=0,0,0,0&width=3200'>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vNEdw_ai6HaU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Les modèles supportés par le fine-tuning**\n",
        "\n",
        "Un réglage fin est actuellement disponible pour les modèles suivants :\n",
        "\n",
        "- `gpt-3.5-turbo-0613`(recommandé)\n",
        "- `babbage-002`\n",
        "- `davinci-002`\n"
      ],
      "metadata": {
        "id": "HZZLTFQU6NYV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tarification**\n",
        "\n",
        "Les coûts de réglage fin sont répartis en deux catégories : le coût de formation initial et le coût d'utilisation :\n",
        "\n",
        "- Formation : 0,008 $ / 1 000 tokens\n",
        "\n",
        "- Entrée d'utilisation : 0,012 $ / 1 000 tokens\n",
        "\n",
        "- Sortie d'utilisation : 0,016 $ / 1 000 tokens\n",
        "\n",
        "Par exemple, un gpt-3.5-turbotravail de mise au point avec un fichier de formation de 100 000 jetons formés pendant 3 époques aurait un coût attendu de 2,40 $.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrnJPzeM6RMG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cas d'utilisation courants**\n",
        "\n",
        "Quelques cas d'utilisation courants dans lesquels un réglage fin peut améliorer les résultats :\n",
        "\n",
        "- Définir le style, le ton, le format ou d'autres aspects qualitatifs\n",
        "- Améliorer la fiabilité pour produire le résultat souhaité\n",
        "- Correction des échecs de suivi d'invites complexes\n",
        "- Gérer de nombreux cas extrêmes de manière spécifique\n",
        "- Exécuter une nouvelle compétence ou une nouvelle tâche difficile à articuler dans une invite\n",
        "\n",
        "Une manière générale d'envisager ces cas consiste à considérer qu'il est plus facile de « montrer, pas de dire ». Dans les sections à venir, nous explorerons comment configurer les données pour un réglage fin et divers exemples dans lesquels le réglage fin améliore les performances par rapport au modèle de base."
      ],
      "metadata": {
        "id": "hXcIYce46TvY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Méthodologie**\n",
        "\n",
        "Le réglage fin améliore l'apprentissage en quelques étapes en s'entraînant sur beaucoup plus d'exemples que ne peut en contenir l'invite, vous permettant d'obtenir de meilleurs résultats sur un grand nombre de tâches. Une fois qu'un modèle a été affiné, vous n'aurez plus besoin de fournir autant d'exemples dans l'invite. Cela permet de réduire les coûts et de permettre des requêtes à faible latence.\n",
        "\n",
        "À un niveau élevé, le réglage fin implique les étapes suivantes :\n",
        "\n",
        "1. Préparer et télécharger les données d'entraînement\n",
        "2. Former un nouveau modèle affiné\n",
        "3. Utilisez votre modèle affiné\n"
      ],
      "metadata": {
        "id": "03tZTidi6WRn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Préparer et télécharger les données d'entraînement**\n",
        "\n",
        "Le réglage fin nécessite un fichier de formation au format JSONL. Chaque ligne du fichier doit contenir un objet JSON avec les clés/valeurs suivantes :\n",
        "\n",
        "- Pour les modèles `ChatCompletion` gt3.5-turbo-0613 :\n",
        "\n",
        "```{\n",
        "    \"message\":[\n",
        "        {\n",
        "            \"role\":\"system\",\n",
        "            \"content\":\"contenu du message\"\n",
        "        },\n",
        "        {\n",
        "            \"role\":\"user\",\n",
        "            \"content\":\"contenu du message\"\n",
        "        },\n",
        "        {\n",
        "            \"role\":\"assistant\",\n",
        "            \"content\":\"contenu du message\"\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "```\n",
        "\n",
        "- Pour les modèles `TextGeneration` :\n",
        "```\n",
        "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
        "```\n",
        "\n",
        "`**Attention**` : **Chaque message doit être placé sur une ligne dans le fichier JSONL. Les messages peuvent être de n'importe quelle longueur.**\n",
        "\n",
        "**Au moins 10 messages sont nécessaires** pour entraîner un modèle. Pour obtenir de meilleurs résultats, nous recommandons d'utiliser au moins 100 messages. Pour les modèles de génération de texte, nous recommandons d'utiliser au moins 1000 messages.\n",
        "\n",
        "Le réglage fin est disponible à partir de la version 0.28.0 de l'[API d'OpenAI](https://platform.openai.com/docs/guides/fine-tuning)."
      ],
      "metadata": {
        "id": "dZD1WxVU6Yt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Structure des données pour l'entrainement d'un modèle ChatCompletion\n",
        "# Moèle concerné : gpt-3.5-turbo-0613 :\n",
        "\n",
        "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What is fine-tuning in machine learning?\"}, {\"role\": \"assistant\", \"content\": \"Fine-tuning in machine learning involves training a pre-existing model on a specific task or dataset to make it more accurate or suitable for that task. It's like giving the model specialized knowledge.\"}]}\n",
        "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"Why is fine-tuning important for AI models?\"}, {\"role\": \"assistant\", \"content\": \"Fine-tuning is crucial because it allows AI models to adapt to specific applications or domains, improving their performance and making them more useful in real-world scenarios.\"}]}\n",
        "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"Can you explain the steps involved in fine-tuning a language model?\"}, {\"role\": \"assistant\", \"content\": \"Certainly! Fine-tuning typically includes preparing training data, training the model on that data, and then using the fine-tuned model for your specific tasks. It's a process of specialization.\"}]}\n",
        "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What models can be fine-tuned for language tasks?\"}, {\"role\": \"assistant\", \"content\": \"Currently, you can fine-tune models like gpt-3.5-turbo-0613, babbage-002, and davinci-002. These models can be adapted to various language-related tasks.\"}]}\n",
        "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"When should I consider using fine-tuning for AI models?\"}, {\"role\": \"assistant\", \"content\": \"You should consider fine-tuning when you need to tailor a model for specific tasks or improve its performance on certain applications. It's a powerful tool for customization.\"}]}\n",
        "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"How can fine-tuning benefit natural language processing (NLP) tasks?\"}, {\"role\": \"assistant\", \"content\": \"Fine-tuning can make NLP models more accurate and contextually aware for tasks like text generation, translation, sentiment analysis, and more. It's all about making the model 'smarter' in a particular domain.\"}]}\n",
        "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What is the role of training data in fine-tuning?\"}, {\"role\": \"assistant\", \"content\": \"Training data is essential for fine-tuning as it helps the model learn from examples. You need a diverse set of conversations or text that reflects the scenarios you want the model to handle.\"}]}\n",
        "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"How many training examples should I provide for fine-tuning?\"}, {\"role\": \"assistant\", \"content\": \"You should provide at least 10 training examples, but for optimal results, it's often recommended to have between 50 and 100 examples. The right number depends on your specific use case.\"}]}\n",
        "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What are the token limits for fine-tuning examples?\"}, {\"role\": \"assistant\", \"content\": \"Each fine-tuning example is limited to 4096 tokens. Make sure the total token count in the messages' content is under 4,000 to fit within this limit.\"}]}\n",
        "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"How can I estimate the costs of fine-tuning?\"}, {\"role\": \"assistant\", \"content\": \"You can estimate the costs by using this formula: base cost per 1,000 tokens * number of tokens in the input file * number of training epochs. This will give you an idea of the expected cost for your fine-tuning job.\"}]}\n",
        "\n",
        "\n",
        "# Structure des données pour l'entrainement d'un modèle Completion\n",
        "# Moèle concerné : babbage-002 et davinci-002\n",
        "\n",
        "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
        "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
        "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}"
      ],
      "metadata": {
        "id": "71dCavKG6H7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Former un nouveau modèle affiné**\n",
        "\n",
        "Une fois les instructions de formation préparées et placées dans un fichier `data.jsonl`, vous pouvez commencer à former votre modèle affiné en préparant les données d'entrainement puis en lançant la formation."
      ],
      "metadata": {
        "id": "RczmCk1P6jHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "\n",
        "# Préparation des données pour l'entrainement d'un modèle ChatCompletion\n",
        "openai.api_key = 'YOUR_API_KEY'\n",
        "response = openai.File.create(\n",
        "  file=open(\"./data.jsonl\", \"rb\"),\n",
        "  purpose='fine-tune'\n",
        ")\n",
        "response"
      ],
      "metadata": {
        "id": "u1ViCFq-6jm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrainement du modèle ChatCompletion sur les serveurs d'OpenAI\n",
        "response_train_model = openai.FineTuningJob.create(\n",
        "    training_file=response['id'],\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    )\n",
        "\n",
        "response_train_model"
      ],
      "metadata": {
        "id": "tYfo6FOf6oGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai.FineTuningJob.list()"
      ],
      "metadata": {
        "id": "H754ewbi6p9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Utilisez votre modèle affiné**\n",
        "\n",
        "Une fois que votre modèle est formé, vous pouvez l'utiliser pour générer des résultats en utilisant l'API comme vous le feriez avec n'importe quel autre modèle. Il est possible de visualiser la courbe d'apprentissage de votre modèle dans le tableau de bord de votre compte :\n",
        "https://platform.openai.com/finetune"
      ],
      "metadata": {
        "id": "-54aZADz6mDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[model['id'] for model in openai.Model().list()[\"data\"]]"
      ],
      "metadata": {
        "id": "kTw9t3p76vfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utilisation du modèle fine-tuné\n",
        "reponse = openai.ChatCompletion.create(\n",
        "    model='ft:gpt-3.5-turbo-0613:dawan::81DNLWeI',\n",
        "    messages=[\n",
        "        {\"role\": \"system\",\"content\": \"Marv is a factual chatbot that is also sarcastic.\"},\n",
        "        {\"role\": \"user\",\n",
        "         \"content\":\"What is fine-tuning in machine learning?\"},\n",
        "    ],\n",
        ")\n",
        "\n",
        "reponse['choices'][0]['message'][\"content\"]"
      ],
      "metadata": {
        "id": "QhgM41x66yIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "\n",
        "# Exercice\n",
        "\n",
        "1. A L'aide de ChatGPT, créez un fichier de données d'entrainement au format JSONL contenant au moins 10 messages puis placez le dans un fichier `data.jsonl` Entrainez ensuite un modèle de chatbot sur un domaine de votre choix.\n",
        "**Astuce** : Copiez le contenu de votre choix (exemple : une page internet) puis demandez à GhatGPT de générer le fichier de données d'entrainement au format JSONL.\n",
        "\n",
        "2. Créez une méthode `openai_chat_finetune()` qui prend en paramètre le nom du fichier de données d'entrainement et retourne le nom du modèle entrainé.\n",
        "\n",
        "### **Exemple de prompt à envoyer à ChatGPT**\n",
        "\n",
        "```\n",
        "\n",
        "A partir du document suivant crée-moi un fichier jsonl contentant des exemples d'échange entre un chatbot et un utilisateur qui sera utilisé pour le fine-tuning d'un model gpt3 pour répondre aux questions de développeurs qui souhaite comprendre comment fonctionne le finetuning. Le jsonl aura la forme suivante :\n",
        "\n",
        "{\"messages\": [{\"role\": \"system\", \"content\": \"definir le rol system\"}, {\"role\": \"user\", \"content\": \"question type posé par un utilisateur\"}, {\"role\": \"assistant\", \"content\": \"Réponse de l'assistant\"}]}\n",
        "\n",
        "\n",
        "Le modèle du fichier json doit avoir cette forme :\n",
        "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn't know that already.\"}]}\n",
        "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"Who wrote 'Romeo and Juliet'?\"}, {\"role\": \"assistant\", \"content\": \"Oh, just some guy named William Shakespeare. Ever heard of him?\"}]}\n",
        "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"}, {\"role\": \"assistant\", \"content\": \"Around 384,400 kilometers. Give or take a few, like that really matters.\"}]}\n",
        "\n",
        "\n",
        "Propose moi à nouveau 20 exemples d'intéraction entre un assistant et un utilisateur au sujet du finetuning en respectant le modèle donné.\n",
        "\n",
        "Voici la documentation à partir de laquelle tu dois créer les exemples : à compléter\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "GlCEJmOv6712"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kaS7n6bl68GU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}